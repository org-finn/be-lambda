import os
import json
import logging
import boto3
from datetime import datetime, timedelta, timezone
from supabase import create_client, Client
from polygon import RESTClient
import pytz

# --- ë¡œê±°, í™˜ê²½ ë³€ìˆ˜, í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ---
logger = logging.getLogger()
logger.setLevel(logging.INFO)

POLYGON_API_KEY = os.environ.get('POLYGON_API_KEY')
SQS_QUEUE_URL = os.environ.get('SQS_QUEUE_URL')
SUPABASE_URL = os.environ.get('SUPABASE_URL')
SUPABASE_KEY = os.environ.get('SUPABASE_KEY')

# í´ë¼ì´ì–¸íŠ¸ëŠ” í•¸ë“¤ëŸ¬ í•¨ìˆ˜ ë°–ì— ì„ ì–¸í•˜ì—¬ ì¬ì‚¬ìš© (ì„±ëŠ¥ ìµœì í™”)
sqs_client = boto3.client('sqs')
polygon_client = RESTClient(POLYGON_API_KEY)
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

def get_tickers_from_supabase():
    """Supabaseì—ì„œ ì²˜ë¦¬í•  ëª¨ë“  í‹°ì»¤ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
    logger.info("Fetching tickers from Supabase.")
    try:
        response = supabase.table('ticker').select('id, code, short_company_name').execute()
        if response.data:
            tickers = [(item['id'], item['code'], item['short_company_name']) for item in response.data]
            logger.info("Found %d tickers to process.", len(tickers))
            return tickers
    except Exception as e:
        logger.exception("Failed to fetch tickers from Supabase.")
    return []

def get_market_status():
    """Polygon APIë¥¼ í˜¸ì¶œí•˜ì—¬ í˜„ì¬ ì‹œì¥ ìƒíƒœê°€ 'OPEN'ì¸ì§€ ì—¬ë¶€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        logger.info("Checking market status...")
        status = polygon_client.get_market_status()
        nasdaq_status = getattr(status.exchanges, 'nasdaq', 'unknown').lower()
        is_open = nasdaq_status == 'open'
        logger.info(f"Market status check complete. Market is {'OPEN' if is_open else 'CLOSED'}.")
        return is_open
    except Exception as e:
        logger.exception("An error occurred during market status check. Assuming market is CLOSED.")
        return False
    
def fetch_articles(published_utc_gte, ticker_code, limit):
    """Polygon APIë¥¼ í˜¸ì¶œí•˜ì—¬ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        response = polygon_client.list_ticker_news(
            ticker=ticker_code,
            limit=limit,
            order='asc',
            published_utc_gte=published_utc_gte
        )
        if response:
            return response
    except Exception as e:
        logger.error("Polygon news API failed for ticker %s: %s", ticker_code or "ALL", e)
    return []

def lambda_handler(event, context):
    """ì—¬ëŸ¬ ì¢…ëª© ì •ë³´ê°€ í¬í•¨ëœ ë‰´ìŠ¤ë¥¼ ê° í‹°ì»¤ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ SQSë¡œ ì „ì†¡í•©ë‹ˆë‹¤."""
    logger.info("Article collection Lambda handler started.")
    
    try:
        is_market_open = get_market_status()
        current_utc_time = datetime.now(timezone.utc).replace(second=0, microsecond=0)
        if is_market_open:
            published_utc_gte = (current_utc_time - timedelta(hours=2)) \
            .strftime('%Y-%m-%dT%H:%M:%SZ') # created_atì´ ì•„ë‹Œ published_date gt ì¡°ê±´ì´ë¯€ë¡œ, ì¢€ ë„‰ë„‰í•˜ê²Œ ë²”ìœ„ë¥¼ ì¡ì•„ì•¼í•¨
        else:
            published_utc_gte = (current_utc_time - timedelta(hours=3)) \
            .strftime('%Y-%m-%dT%H:%M:%SZ')
        logger.info("Will fetch article %s ~ %s of UTC", published_utc_gte, current_utc_time.strftime('%Y-%m-%dT%H:%M:%SZ'))
        
        prediction_date = datetime.now(pytz.timezone('US/Eastern')) \
            .replace(second=0, microsecond=0) # ë°ì´í„° ê°„ ë‚ ì§œ í†µì¼ì„ ìœ„í•´ ì‚¬ìš©
        
        articles_by_ticker = {}
        seen_article_ids = set()
        
        # Supabaseì—ì„œ í‹°ì»¤ ì •ë³´ë¥¼ ë¯¸ë¦¬ ê°€ì ¸ì™€ ë¹ ë¥¸ ì¡°íšŒë¥¼ ìœ„í•œ ë§µê³¼ set ìƒì„±
        all_tickers_from_db = get_tickers_from_supabase()
        ticker_info_map = {
            ticker[1]: {'id': ticker[0], 'name': ticker[2]} for ticker in all_tickers_from_db
        }
        supported_tickers_set = set(ticker_info_map.keys())

        # ì²˜ë¦¬í•  ëª¨ë“  ë‰´ìŠ¤ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸ (ì „ì²´ ë‰´ìŠ¤ + í‹°ì»¤ë³„ ë‰´ìŠ¤)
        articles_to_process = []
        
        # 1. ì „ì²´ ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘
        logger.info("Fetching general articles...")
        articles_to_process.extend(fetch_articles(published_utc_gte, None, 50))
        
        # 2. í‹°ì»¤ë³„ ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘
        if all_tickers_from_db:
            for _, ticker_code, _ in all_tickers_from_db:
                logger.info("Fetching articles for %s...", ticker_code)
                articles_to_process.extend(fetch_articles(published_utc_gte, ticker_code, 30))

        # 3. ìˆ˜ì§‘ëœ ëª¨ë“  ë‰´ìŠ¤ë¥¼ ìˆœíšŒí•˜ë©° í‹°ì»¤ë³„ë¡œ ê·¸ë£¹í™”
        for article in articles_to_process:
            if article.id in seen_article_ids:
                continue

            sentiment, reasoning = None, None
            if hasattr(article, 'insights') and article.insights:
                first_insight = article.insights[0]
                sentiment = first_insight.sentiment
                reasoning = first_insight.sentiment_reasoning

            # â­ï¸ ê³µí†µ í˜ì´ë¡œë“œ ìƒì„± (í‹°ì»¤ ì •ë³´ ì œì™¸)
            base_payload = {
                'published_date': article.published_utc, 'title': article.title,
                'description': article.description, 'article_url': article.article_url,
                'thumbnail_url': getattr(article, 'image_url', None), 'author': article.author,
                'sentiment': sentiment, 'reasoning': reasoning
            }
            
            # ì´ ê¸°ì‚¬ì™€ ì—°ê´€ëœ ëª¨ë“  í‹°ì»¤ë¥¼ ì°¾ìŒ
            related_tickers = set(article.tickers)
            if hasattr(article, 'insights') and article.insights:
                for insight in article.insights:
                    related_tickers.add(insight.ticker)

            is_related_to_supported_ticker = False
            if related_tickers:
                # ê° ì—°ê´€ í‹°ì»¤ ê·¸ë£¹ì— ë‰´ìŠ¤ ì¶”ê°€
                for ticker_code in related_tickers:
                    # ìš°ë¦¬ ì„œë¹„ìŠ¤ì—ì„œ ì§€ì›í•˜ëŠ” í‹°ì»¤ì¸ì§€ í™•ì¸
                    if ticker_code in supported_tickers_set:
                        payload = base_payload.copy()
                        # â­ï¸ ì›ë³¸ IDì™€ í‹°ì»¤ ì½”ë“œë¥¼ ì¡°í•©í•˜ì—¬ ìƒˆë¡œìš´ ê³ ìœ  ID ìƒì„±
                        payload['distinct_id'] = f"{article.id}-{ticker_code}"
                        articles_by_ticker.setdefault(ticker_code, []).append(payload)
                        is_related_to_supported_ticker = True
            
            # ì—°ê´€ í‹°ì»¤ê°€ ì—†ê±°ë‚˜, ìˆë”ë¼ë„ ëª¨ë‘ ì§€ì›í•˜ì§€ ì•ŠëŠ” í‹°ì»¤ì¼ ê²½ìš° GENERAL ê·¸ë£¹ì— ì¶”ê°€
            if not is_related_to_supported_ticker:
                payload = base_payload.copy()
                # ì›ë³¸ ID ì‚¬ìš©(GENERALì— ì†í•˜ëŠ” ì•„í‹°í´ë“¤ì€ distinct_idë¥¼ ê·¸ëŒ€ë¡œ ì“°ë¯€ë¡œ ë¬´ì¡°ê±´ ìœ ë‹ˆí¬í•¨)
                payload['distinct_id'] = article.id
                articles_by_ticker.setdefault('GENERAL', []).append(payload)

            seen_article_ids.add(article.id)

        # 4. ìˆ˜ì§‘ëœ ë‰´ìŠ¤ë¥¼ SQS íë¡œ ì „ì†¡
        if not articles_by_ticker:
            logger.info("No new articles to process.")
            return {'statusCode': 200, 'body': 'No new articles found.'}

        # SQSë¡œ ë³´ë‚¼ ì´ ê¸°ì‚¬ ìˆ˜ ê³„ì‚°
        total_article_count = sum(len(articles) for articles in articles_by_ticker.values())
        
        ticker_groups_to_send = list(articles_by_ticker.keys())
        logger.info(
            "Sending %d unique articles for %d tickers/groups to SQS queue: %s",
            total_article_count, # ğŸ‘ˆ ì´ ê¸°ì‚¬ ìˆ˜ ë¡œê·¸ ì¶”ê°€
            len(ticker_groups_to_send),
            ticker_groups_to_send
        )
        
        entries_to_send = []
        for ticker_code, articles in articles_by_ticker.items():
            ticker_info = ticker_info_map.get(ticker_code, {})
            message_body = {
                'ticker_code': ticker_code,
                'ticker_id': ticker_info.get('id'),
                'short_company_name': ticker_info.get('name'),
                'articles': articles,
                'is_market_open': is_market_open,
                'prediction_date' : prediction_date.isoformat(),
                'created_at': datetime.now(pytz.timezone("Asia/Seoul")).isoformat()
            }
            entries_to_send.append({
                'Id': ticker_code.replace('.', '-'), # SQS IDì— '.' í—ˆìš© ì•ˆë¨
                'MessageBody': json.dumps(message_body)
            })
        
        # ë°°ì¹˜ ë‹¨ìœ„ ì‚½ì… (ìµœëŒ€ 10ê°œ ê·¸ë£¹ì”©)
        for i in range(0, len(entries_to_send), 10):
            batch = entries_to_send[i:i+10]
            sqs_client.send_message_batch(QueueUrl=SQS_QUEUE_URL, Entries=batch)
        
        logger.info("âœ… Successfully sent all article groups to SQS.")
        
    except Exception as e:
        logger.exception("A critical error occurred in the lambda handler.")
        raise e

    return {
        'statusCode': 200,
        'body': f'Successfully processed and sent news for {len(articles_by_ticker)} groups to SQS.'
    }