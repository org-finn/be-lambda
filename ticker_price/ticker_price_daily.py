import json
import os
import boto3
import logging
from supabase import create_client, Client
from datetime import date, timedelta, datetime
import requests
import json
import time
import psycopg2
from psycopg2.extras import execute_batch
from polygon import RESTClient
import pytz
import exchange_calendars as xcals

logger = logging.getLogger()
logger.setLevel(logging.INFO)

POLYGON_API_KEY = os.environ.get('POLYGON_API_KEY')

SQS_QUEUE_URL = os.environ.get('SQS_QUEUE_URL')

# í‹°ì»¤ ì¡°íšŒ ìš©ë„ë¡œ ì‚¬ìš©
SUPABASE_URL = os.environ.get('SUPABASE_URL')
SUPABASE_KEY = os.environ.get('SUPABASE_KEY')

sqs_client = boto3.client('sqs')
ssm_client = boto3.client('ssm')
polygon_client = RESTClient(POLYGON_API_KEY)
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
US_CALENDAR = xcals.get_calendar("XNYS") # âœ… ë¯¸êµ­ ì¦ì‹œ ìº˜ë¦°ë”
US_EASTERN_TZ = pytz.timezone("America/New_York") # âœ… ë¯¸êµ­ ë™ë¶€ íƒ€ì„ì¡´


def get_tickers_from_parameter_store():
    """Parameter Storeì—ì„œ ì €ì¥ëœ í‹°ì»¤ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    logger.info("Fetching tickers from Parameter Store.")
    try:
        param = ssm_client.get_parameter(Name='/articker/tickers')
        
        # ì €ì¥ëœ JSON ë¬¸ìì—´ì„ íŒŒì‹±
        cached_data = json.loads(param['Parameter']['Value'])
        all_tickers = cached_data.get('tickers', [])

        filtered_tickers = [
            (item[0], item[1]) for item in all_tickers
        ]
        
        return filtered_tickers
    
    except ssm_client.exceptions.ParameterNotFound:
        logger.info("No ticker cache found in Parameter Store.")
        return None
    except Exception as e:
        logger.exception("Failed to get tickers from Parameter Store.")
        return None
    
def get_tickers_from_supabase():
    """Supabaseì—ì„œ ì²˜ë¦¬í•  ëª¨ë“  í‹°ì»¤ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤."""
    logger.info("Fetching tickers from Supabase.")
    
    try:
        response = supabase.table('ticker').select('id, code').execute()
        if response.data:
            tickers = [(item['id'], item['code']) for item in response.data]
            logger.info("Found %d tickers to process.", len(tickers))
            return tickers
        else:
            # ë°ì´í„°ê°€ ì—†ëŠ” ê²ƒì€ ì¬ì‹œë„í•  í•„ìš”ê°€ ì—†ëŠ” ì •ìƒ ìƒí™©ì¼ ìˆ˜ ìˆìŒ
            logger.warning("No tickers found from Supabase.")
            return []
    except Exception as e:
        # DB ì—°ê²° ì‹¤íŒ¨ ë“±ì€ ì¬ì‹œë„ê°€ í•„ìš”í•œ ì˜¤ë¥˜ì´ë¯€ë¡œ ì˜ˆì™¸ ë°œìƒ
        logger.exception("Failed to fetch tickers from Supabase.")
        raise e


def fetch_previous_day_data(ticker_code):
    """Polygon APIë¥¼ í˜¸ì¶œí•˜ì—¬ íŠ¹ì • í‹°ì»¤ì˜ ì „ë‚  ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        response = polygon_client.get_previous_close_agg(
            ticker_code,
            adjusted="true",
        )
        
        if response and len(response) == 1:
            # vars()ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ì˜ ì²« ë²ˆì§¸ í•­ëª©ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            return vars(response[0])
        
    except Exception as e:
        logger.error("Polygon client failed for ticker %s: %s", ticker_code, e)
    return None

def process_ticker(ticker_info):
    """ë‹¨ì¼ í‹°ì»¤ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ SQS ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ê°€ê³µí•©ë‹ˆë‹¤."""
    ticker_id = ticker_info[0]
    ticker_code = ticker_info[1]
    result = fetch_previous_day_data(ticker_code)
    
    if result:
        price_date = datetime.fromtimestamp(result['timestamp'] / 1000).strftime('%Y-%m-%d')
        
        # SQS ë©”ì‹œì§€ë¡œ ë³´ë‚¼ ë°ì´í„° ê°ì²´ ìƒì„±
        # ë“±ë½ë¥  ê³„ì‚°ì€ ì´ì œ SQS ë©”ì‹œì§€ë¥¼ ë°›ëŠ” ìª½ì—ì„œ ë‹´ë‹¹
        message_payload = {
            'ticker_id': ticker_id,
            'ticker_code': ticker_code,
            'price_date': price_date,
            'open': result.get('open'),
            'high': result.get('high'),
            'low': result.get('low'),
            'close': result.get('close'),
            'volume': int(result.get('volume'))
        }
        logger.info("Successfully fetched data for %s", ticker_code)
        return message_payload
    else:
        logger.warning("Could not retrieve price data for %s.", ticker_code)
        return None

def lambda_handler(event, context):
    logger.info("Lambda handler started: Fetching stock prices to send to SQS.")
    try:
        # 0. íœ´ì¥ì¼ í™•ì¸ ë¡œì§
        # ëŒë‹¤ ì‹¤í–‰ ì‹œì ì˜ ë¯¸êµ­ ë™ë¶€ ì‹œê°„ ê¸°ì¤€ 'ì–´ì œ' ë‚ ì§œ ê³„ì‚°
        now_et = datetime.now(US_EASTERN_TZ)
        previous_day_str = now_et.strftime('%Y-%m-%d')

        # ì–´ì œ(ë™ë¶€ ì‹œê°„ ê¸°ì¤€ ì˜¤ëŠ˜)ê°€ ê±°ë˜ì¼(session)ì´ ì•„ë‹ˆì—ˆëŠ”ì§€ í™•ì¸
        if not US_CALENDAR.is_session(previous_day_str):
            logger.warning(f"Skipping execution because the previous day ({previous_day_str}) was a market holiday.")
            return {'statusCode': 200, 'body': f'Skipped: {previous_day_str} was a holiday.'}
        
        # 1. ì²˜ë¦¬í•  í‹°ì»¤ ëª©ë¡ ì¡°íšŒ
        tickers = get_tickers_from_parameter_store() # ì„  íŒŒë¼ë¯¸í„° ì¡°íšŒ
        if tickers is None:
            tickers = get_tickers_from_supabase() # ì•ˆì „ì¥ì¹˜ë¡œ dbì—ì„œ ì¡°íšŒ
        if not tickers:
            return {'statusCode': 200, 'body': 'No tickers to process.'}
            
        messages_to_send = []

        # 2. ìˆœì°¨ ì²˜ë¦¬ë¥¼ í†µí•´ ê° í‹°ì»¤ì˜ ë°ì´í„°ë¥¼ APIë¡œ ê°€ì ¸ì˜¤ê¸°
        logger.info("Starting sequential processing for %d tickers...", len(tickers))
        for ticker in tickers:
            result = process_ticker(ticker)
            if result:
                messages_to_send.append(result)

        # 3. ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ SQS íë¡œ ì „ì†¡
        if messages_to_send:
            logger.info("Sending %d messages to SQS queue...", len(messages_to_send))
            
            # SQSëŠ” ìµœëŒ€ 10ê°œì”© ë©”ì‹œì§€ë¥¼ ë¬¶ì–´ ë³´ë‚¼ ìˆ˜ ìˆìŒ (Batch ì „ì†¡)
            for i in range(0, len(messages_to_send), 10):
                batch = messages_to_send[i:i+10]
                entries = [
                    # IdëŠ” ë°°ì¹˜ ë‚´ì—ì„œ ìœ ë‹ˆí¬í•´ì•¼ í•˜ë¯€ë¡œ ticker_code ì‚¬ìš©
                    {'Id': msg['ticker_code'], 'MessageBody': json.dumps(msg)} 
                    for msg in batch
                ]
                
                try:
                    sqs_client.send_message_batch(
                        QueueUrl=SQS_QUEUE_URL,
                        Entries=entries
                    )
                except Exception as e:
                    logger.exception("Failed to send batch to SQS: %s", e)
                    return {'statusCode': 500, 'body': "Failed to send batch to SQS"}
            
            logger.info("âœ… Successfully sent all message batches to SQS.")
        
        processed_count = len(messages_to_send)
        if processed_count == 0 and len(tickers) > 0:
            raise Exception(f"Processed 0 tickers out of {len(tickers)}. All API calls might have failed.")
            
        logger.info("Lambda handler finished successfully.")
        return {
            'statusCode': 200,
            'body': f'Successfully processed and sent {processed_count} tickers to SQS.'
        }
    
    except Exception as e:
        logger.exception("A critical error occurred in the lambda handler.")
        # ğŸš¨ ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œ Lambda ì‹¤í–‰ì„ 'ì‹¤íŒ¨'ë¡œ AWSì— ì•Œë¦½ë‹ˆë‹¤.
        raise e
